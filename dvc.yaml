# =====================================================================
# DVC Pipeline Configuration
# Data Version Control para MLOps - Housing Price Prediction
# =====================================================================
# Pipeline simplificado de 3 etapas:
# 1. data_ingestion: Descarga de datos desde Kaggle
# 2. data_preparation: Limpieza, splitting y escalado
# 3. model_train: Entrenamiento de 3 experimentos XGBoost con SHAP + MLflow
# =====================================================================

stages:
  # ===================================================================
  # Stage 1: Data Ingestion
  # Descarga datos desde Kaggle, genera dataset limpio y reporte EDA
  # ===================================================================
  data_ingestion:
    cmd: python src/data_ingestion.py
    deps:
      - src/data_ingestion.py
    outs:
      - data/raw/HousingData.csv
      - data/reports/

  # ===================================================================
  # Stage 2: Data Preparation
  # Limpieza, transformación y feature engineering
  # GENERA: StandardScaler para producción (models/production/latest/scaler.pkl)
  # ===================================================================
  data_preparation:
    cmd: python src/data_preparation.py
    deps:
      - src/data_preparation.py
      - data/raw/HousingData.csv
    params:
      - data_ingestion
      - preprocessing
      - data_preparation
    outs:
      - data/processed/
      - models/standard_scaler.pkl
      - models/production/latest/scaler.pkl

  # ===================================================================
  # Stage 3: Model Training
  # Entrenamiento de 3 experimentos XGBoost con SHAP + MLflow
  # USA MLFLOW AUTOLOGGING - Todo se guarda en mlruns/
  # EXPORTA AUTOMÁTICAMENTE a models/production/latest/
  # Experimentos:
  #   1. Hyperparameter Tuning (todas las features)
  #   2. Important Features (SHAP percentil 20 + default params)
  #   3. Tuning on Selected Features (features de exp2 + new tuning)
  # Los modelos, artifacts, métricas y parámetros se guardan automáticamente
  # en mlruns/ gracias a mlflow.xgboost.autolog()
  # El mejor modelo se exporta automáticamente para la API
  # NOTA: El scaler.pkl viene desde data_preparation (no se genera aquí)
  # ===================================================================
  model_train:
    cmd: python src/model_train.py
    deps:
      - src/model_train.py
      - data/processed/train.csv
      - data/processed/test.csv
    params:
      - data_ingestion.target_column
      - preprocessing.processed_data_dir
      - mlflow.tracking_uri
      - mlflow.experiment_name
    outs:
      # Modelo exportado para la API (scaler.pkl viene de data_preparation)
      - models/production/latest/model.pkl
      - models/production/latest/metadata.json

  # ===================================================================
  # Stage 4: Model Registration
  # Registro del mejor modelo en MLflow Model Registry.
  # Lee model_info.json (generado por model_train) y registra el modelo
  # en MLflow Model Registry con el stage especificado (Production, Staging).
  # Archiva automáticamente versiones antiguas del mismo stage.
  # ===================================================================
  model_register:
    cmd: python src/model_register.py --stage Production
    deps:
      - src/model_register.py
      - models/model_info.json
