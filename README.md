# ğŸ  Housing Price Prediction - MLOps Production System

[![ML Pipeline CI/CD](https://github.com/marestrepohi/meli-mlops-mateo-restrepo/actions/workflows/ml-pipeline.yml/badge.svg)](https://github.com/marestrepohi/meli-mlops-mateo-restrepo/actions/workflows/ml-pipeline.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![MLflow](https://img.shields.io/badge/MLflow-2.8+-orange.svg)](https://mlflow.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

> Sistema completo y producciÃ³n-ready de MLOps para predicciÃ³n de precios de viviendas. ImplementaciÃ³n agnÃ³stica a la nube con tecnologÃ­as open-source, pipeline reproducible, API REST, monitoreo continuo y CI/CD automatizado.

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa una soluciÃ³n end-to-end de Machine Learning Operations (MLOps) para **predecir precios de viviendas** utilizando el dataset Boston Housing. El sistema estÃ¡ diseÃ±ado siguiendo las mejores prÃ¡cticas de la industria y es completamente **agnÃ³stico a proveedores cloud**, utilizando Ãºnicamente herramientas open-source y self-hosted.

### ğŸ¯ Objetivos del Sistema

1. **Reproducibilidad**: Pipeline completamente versionado y automatizable
2. **Portabilidad**: TecnologÃ­as open-source, sin dependencias de servicios cloud gestionados
3. **Monitoreo**: Tracking de performance, latencia y data drift en tiempo real
4. **Escalabilidad**: Arquitectura containerizada lista para orquestadores (K8s, Docker Swarm)
5. **Mantenibilidad**: CÃ³digo modular, documentado y con tests comprehensivos

---

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PIPELINE DE ENTRENAMIENTO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Data Ingestion  â†’  2. Data Preparation  â†’  3. Model Train  â”‚
â”‚     (Kaggle API)        (Clean + Scale)         (XGBoost + MLflow)â”‚
â”‚         â†“                      â†“                        â†“         â”‚
â”‚   data/raw/           data/processed/          mlruns/ + models/  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARTEFACTOS DE PRODUCCIÃ“N                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  models/production/latest/                                       â”‚
â”‚    â”œâ”€â”€ model.pkl         (XGBoost model)                        â”‚
â”‚    â”œâ”€â”€ scaler.pkl        (StandardScaler)                       â”‚
â”‚    â””â”€â”€ metadata.json     (Features, metrics, run_id)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API REST (FastAPI)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Endpoints:                                                      â”‚
â”‚    â€¢ POST /predict          (Individual prediction)             â”‚
â”‚    â€¢ POST /predict/batch    (Batch predictions)                 â”‚
â”‚    â€¢ GET  /health           (Health check)                      â”‚
â”‚    â€¢ GET  /metrics          (Performance metrics)               â”‚
â”‚    â€¢ GET  /drift            (Data drift detection)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITOREO Y OBSERVABILIDAD                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Prediction logging                                            â”‚
â”‚  â€¢ Latency tracking (p50, p95, p99)                             â”‚
â”‚  â€¢ Data drift detection (statistical tests)                     â”‚
â”‚  â€¢ Model performance monitoring                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

- Python 3.10+
- Docker & Docker Compose (opcional pero recomendado)
- Git
- Make (opcional, para usar Makefile)

### OpciÃ³n 1: Setup AutomÃ¡tico (Recomendado)

```bash
# Clonar repositorio
git clone https://github.com/marestrepohi/meli-mlops-mateo-restrepo.git
cd meli-mlops-mateo-restrepo

# Ejecutar setup automÃ¡tico
bash setup.sh

# O usando Make
make setup
```

### OpciÃ³n 2: Setup Manual

```bash
# 1. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Ejecutar pipeline de entrenamiento
dvc repro

# 4. Iniciar API
uvicorn api.main:app --reload --port 8000

# 5. (Opcional) Ver experimentos en MLflow UI
mlflow ui --port 5000
```

### OpciÃ³n 3: Docker (ProducciÃ³n)

```bash
# Iniciar todo el stack (API + MLflow UI)
docker-compose up -d

# Verificar que estÃ¡ corriendo
curl http://localhost:8000/health
```

---

## ğŸ“‚ Estructura del Proyecto

```
meli-mlops-mateo-restrepo/
â”œâ”€â”€ api/                          # API REST
â”‚   â”œâ”€â”€ main.py                   # FastAPI application
â”‚   â”œâ”€â”€ monitoring.py             # Monitoring & drift detection
â”‚   â””â”€â”€ security.py               # Authentication & rate limiting
â”‚
â”œâ”€â”€ src/                          # Pipeline de ML
â”‚   â”œâ”€â”€ data_ingestion.py         # Descarga de datos (Kaggle)
â”‚   â”œâ”€â”€ data_preparation.py       # Limpieza, split, scaling
â”‚   â”œâ”€â”€ model_train.py            # Entrenamiento (3 experimentos XGBoost)
â”‚   â”œâ”€â”€ model_register.py         # MLflow Model Registry (opcional)
â”‚   â””â”€â”€ config.py                 # ConfiguraciÃ³n centralizada
â”‚
â”œâ”€â”€ tests/                        # Tests comprehensivos
â”‚   â”œâ”€â”€ test_api.py               # Tests de API (endpoints, security)
â”‚   â”œâ”€â”€ test_pipeline.py          # Tests de pipeline (data, training)
â”‚   â”œâ”€â”€ test_model.py             # Tests de modelo (quality, performance)
â”‚   â””â”€â”€ conftest.py               # Fixtures de pytest
â”‚
â”œâ”€â”€ data/                         # Datos versionados con DVC
â”‚   â”œâ”€â”€ raw/                      # Datos originales
â”‚   â”œâ”€â”€ processed/                # Datos preprocesados
â”‚   â”œâ”€â”€ predictions/              # Predicciones guardadas
â”‚   â””â”€â”€ reports/                  # Reportes EDA
â”‚
â”œâ”€â”€ models/                       # Modelos en producciÃ³n
â”‚   â””â”€â”€ production/latest/        # Ãšltimo modelo productizado
â”‚       â”œâ”€â”€ model.pkl
â”‚       â”œâ”€â”€ scaler.pkl
â”‚       â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ mlruns/                       # MLflow tracking (local)
â”‚   â””â”€â”€ <experiment_id>/          # Experimentos, runs, artifacts
â”‚
â”œâ”€â”€ .github/workflows/            # CI/CD con GitHub Actions
â”‚   â”œâ”€â”€ ml-pipeline.yml           # Pipeline principal (train + test + deploy)
â”‚   â”œâ”€â”€ api-tests.yml             # Tests de API
â”‚   â””â”€â”€ scheduled-retrain.yml     # Reentrenamiento automÃ¡tico semanal
â”‚
â”œâ”€â”€ dvc.yaml                      # Pipeline DVC (reproducibilidad)
â”œâ”€â”€ params.yaml                   # HiperparÃ¡metros y configuraciÃ³n
â”œâ”€â”€ docker-compose.yml            # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                    # Imagen Docker para API
â”œâ”€â”€ Makefile                      # Comandos de desarrollo
â”œâ”€â”€ requirements.txt              # Dependencias Python
â””â”€â”€ README.md                     # Este archivo
```

---

## ğŸ”¬ Pipeline de Entrenamiento

El pipeline estÃ¡ implementado con **DVC** para reproducibilidad completa:

### Etapa 1: Data Ingestion (`src/data_ingestion.py`)

- Descarga dataset Boston Housing desde Kaggle
- Genera reporte EDA automÃ¡tico con `ydata-profiling`
- Output: `data/raw/HousingData.csv`

### Etapa 2: Data Preparation (`src/data_preparation.py`)

- **Limpieza**: Manejo de valores nulos (mediana/moda)
- **Split**: Train/Test (80/20) con seed fijo
- **Scaling**: StandardScaler (importante: guarda scaler puro, no dict)
- **Outputs**: 
  - `data/processed/{train,test,X_train,X_test,y_train,y_test}.csv`
  - `models/production/latest/scaler.pkl` (para API)

### Etapa 3: Model Training (`src/model_train.py`)

**3 Experimentos XGBoost con MLflow Autologging:**

1. **Hyperparameter Tuning** (todas las features)
   - RandomizedSearchCV con 50 iteraciones
   - 5-fold cross-validation
   
2. **Important Features** (selecciÃ³n con SHAP)
   - Selecciona top features (percentil 20)
   - HyperparÃ¡metros default
   
3. **Tuning on Selected Features**
   - RandomizedSearchCV en features seleccionadas
   - Mejor balance performance/complejidad

**Artifacts generados automÃ¡ticamente:**
- Feature importance (weight, gain, cover)
- SHAP values y plots
- Residuals plot
- Predictions vs Actuals
- Model signature (MLflow)

**Output:** Mejor modelo exportado a `models/production/latest/`

### EjecuciÃ³n del Pipeline

```bash
# Ejecutar pipeline completo
dvc repro

# Ejecutar stages especÃ­ficos
dvc repro data_preparation
dvc repro model_train

# Ver DAG del pipeline
dvc dag
```

---

## ğŸŒ API REST

FastAPI-based REST API con validaciÃ³n automÃ¡tica, documentaciÃ³n Swagger y monitoreo.

### Endpoints Principales

#### `POST /predict` - PredicciÃ³n Individual

Realiza predicciÃ³n para una vivienda.

**Request:**
```json
{
  "CRIM": 0.00632,
  "NOX": 0.538,
  "RM": 6.575,
  "AGE": 65.2,
  "DIS": 4.09,
  "RAD": 1.0,
  "TAX": 296.0,
  "PTRATIO": 15.3,
  "B": 396.90,
  "LSTAT": 4.98
}
```

**Response:**
```json
{
  "prediction": 24.5,
  "model_name": "boston_housing_xgboost",
  "model_version": "1.0",
  "inference_time": 15.2,
  "features_used": ["CRIM", "NOX", "RM", ...]
}
```

**Con autenticaciÃ³n (producciÃ³n):**
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -H "X-API-Key: demo-key-123" \
  -d '{"CRIM": 0.00632, "NOX": 0.538, ...}'
```

#### `POST /predict/batch` - PredicciÃ³n en Batch

MÃºltiples predicciones en una sola request.

**Request:**
```json
{
  "instances": [
    {"CRIM": 0.00632, "NOX": 0.538, ...},
    {"CRIM": 0.02731, "NOX": 0.469, ...}
  ]
}
```

#### `GET /health` - Health Check

VerificaciÃ³n del estado del servicio y modelos cargados.

#### `GET /metrics` - MÃ©tricas de Performance

EstadÃ­sticas de uso, latencia y predicciones.

#### `GET /drift` - DetecciÃ³n de Data Drift

Compara distribuciÃ³n actual vs baseline para detectar drift.

### DocumentaciÃ³n Interactiva

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Seguridad Implementada

1. **API Key Authentication**: Header `X-API-Key`
2. **Rate Limiting**: 100 requests/min por API key
3. **Input Validation**: Pydantic schemas con rangos vÃ¡lidos
4. **CORS**: Configurado para desarrollo (ajustar en producciÃ³n)

---

## ğŸ“Š Monitoreo

Sistema de monitoreo completo implementado en `api/monitoring.py`:

### MÃ©tricas Rastreadas

1. **Performance Metrics**
   - Total de predicciones
   - Uptime del servicio
   - Predicciones por hora

2. **Latency Metrics**
   - Tiempo promedio de inferencia
   - Percentiles: p50, p95, p99
   - Tiempo mÃ¡ximo de inferencia

3. **Prediction Statistics**
   - Media, mediana, std de predicciones
   - Min/max values
   - DistribuciÃ³n de predicciones

4. **Data Drift Detection**
   - ComparaciÃ³n vs baseline (mean Â± 2Ïƒ)
   - Drift score por feature
   - Alertas automÃ¡ticas

### Configurar Baseline para Drift Detection

```python
from api.monitoring import monitor

# Configurar baseline con datos histÃ³ricos
historical_predictions = [20.5, 22.3, 19.8, ...]
historical_features = {
    'CRIM': [0.1, 0.2, ...],
    'RM': [6.0, 6.5, ...]
}

monitor.set_baseline(historical_predictions, historical_features)
```

---

## ğŸ³ Despliegue

### Docker

```bash
# Build imagen
docker build -t housing-api:latest .

# Run container
docker run -p 8000:8000 housing-api:latest
```

### Docker Compose (Stack Completo)

```bash
# Iniciar servicios (API + MLflow UI)
docker-compose up -d

# Ver logs
docker-compose logs -f api

# Detener servicios
docker-compose down
```

**Servicios disponibles:**
- API: http://localhost:8000
- MLflow UI: http://localhost:5000
- Docs: http://localhost:8000/docs

### GitHub Actions CI/CD

3 workflows automatizados:

1. **`ml-pipeline.yml`** (on push to main)
   - Setup Python environment
   - Run tests
   - Execute DVC pipeline
   - Build Docker image
   - Deploy to production

2. **`api-tests.yml`** (on PR)
   - Unit tests
   - Integration tests
   - API tests

3. **`scheduled-retrain.yml`** (cron: weekly)
   - Reentrenamiento automÃ¡tico
   - EvaluaciÃ³n de modelo nuevo
   - Deploy si mejora performance

---

## ğŸ§  Decisiones TÃ©cnicas

### 1. Stack TecnolÃ³gico

**Â¿Por quÃ© estas tecnologÃ­as?**

| Herramienta | JustificaciÃ³n |
|-------------|---------------|
| **XGBoost** | Mejor performance para datos tabulares, interpretable con SHAP |
| **FastAPI** | Async by default, validaciÃ³n automÃ¡tica (Pydantic), docs autogeneradas |
| **MLflow** | Open-source, tracking completo, Model Registry, serving capabilities |
| **DVC** | Versionado de datos/modelos, reproducibilidad, integraciÃ³n Git |
| **Docker** | Portabilidad, reproducibilidad del entorno, fÃ¡cil deployment |
| **GitHub Actions** | CI/CD nativo de GitHub, gratuito, gran ecosistema de actions |

### 2. Arquitectura de Pipeline

**SeparaciÃ³n clara de responsabilidades:**

- `data_preparation.py` â†’ **ÃšNICO responsable del scaler**
  - Guarda `scaler.pkl` directamente en `models/production/latest/`
  - Evita duplicaciÃ³n y mantiene coherencia
  
- `model_train.py` â†’ **SOLO entrena y exporta modelo**
  - No duplica el scaler
  - Verifica que scaler exista antes de exportar

**Beneficios:**
- No hay duplicaciÃ³n de lÃ³gica
- El scaler siempre estÃ¡ sincronizado con los datos
- Pipeline mÃ¡s eficiente (no re-calcula scaler)

### 3. MLflow Autologging vs Manual Logging

**Elegimos Autologging porque:**
- âœ… Logs automÃ¡ticos de hiperparÃ¡metros
- âœ… Captura model signature automÃ¡ticamente
- âœ… Guarda pip requirements
- âœ… Menos cÃ³digo, menos errores

**Lo que agregamos manualmente:**
- Feature importance plots
- SHAP values y waterfall plots
- Custom metrics (CV scores, test metrics)
- Metadata JSON para API

### 4. Scaler: Dict vs StandardScaler Puro

**Problema original:**
```python
# âŒ ANTES: Guardaba dict
joblib.dump({
    "escalador": StandardScaler(),
    "nombres_caracteristicas": [...],
    "nombre_objetivo": "MEDV"
}, "scaler.pkl")
```

**SoluciÃ³n:**
```python
# âœ… AHORA: Guarda StandardScaler puro
joblib.dump(prep.escalador, "models/production/latest/scaler.pkl")
```

**JustificaciÃ³n:**
- API necesita `.transform()` directamente
- MÃ¡s simple, menos overhead
- Metadata de features ya estÃ¡ en `metadata.json`

### 5. ValidaciÃ³n de Inputs (Pydantic)

**Features requeridas vs opcionales:**

- **Requeridas** (10): Las que el modelo usa â†’ `Field(...)`
- **Opcionales** (3): Las que el scaler necesita pero modelo no usa â†’ `Field(0.0)`

```python
class PredictionInput(BaseModel):
    # Requeridas por el modelo
    CRIM: float = Field(..., ge=0.0, le=100.0)
    RM: float = Field(..., ge=3.0, le=9.0)
    
    # Opcionales (para scaler)
    ZN: Optional[float] = Field(0.0, ge=0.0, le=100.0)
```

**Beneficios:**
- ValidaciÃ³n automÃ¡tica de rangos
- DocumentaciÃ³n autogenerada
- Mejor experiencia de usuario (errores claros)

### 6. Seguridad: API Keys + Rate Limiting

**ImplementaciÃ³n simple pero efectiva:**

- API Keys en variable de entorno (configurable)
- Rate limiter in-memory (100 req/min)
- Para producciÃ³n escalable â†’ usar Redis

**Alternativas consideradas:**
- OAuth2: Sobrekill para este caso
- JWT: MÃ¡s complejo sin beneficio claro
- Sin auth: âŒ No aceptable en producciÃ³n

### 7. Monitoreo: In-Memory vs Persistente

**SoluciÃ³n actual: In-memory (deque)**
- âœ… RÃ¡pido, sin overhead
- âœ… Suficiente para MVP
- âŒ Se pierde al restart

**Para producciÃ³n:**
- Usar Prometheus + Grafana
- Persistir mÃ©tricas en TimeSeries DB
- Alerting con thresholds configurables

---

## ğŸ§ª Testing

Suite completa de tests con pytest:

```bash
# Todos los tests
pytest

# Con coverage
pytest --cov=src --cov=api --cov-report=html

# Solo tests rÃ¡pidos
pytest -m "not slow"

# Solo tests de API
pytest tests/test_api.py -v

# Solo tests de pipeline
pytest tests/test_pipeline.py -v
```

### CategorÃ­as de Tests

1. **Unit Tests** (`test_pipeline.py`)
   - Data cleaning
   - Train/test splitting
   - Scaler fitting/transforming

2. **Integration Tests** (`test_api.py`)
   - Endpoints (health, predict, batch)
   - Input validation
   - Error handling
   - Performance (latency)

3. **Model Quality Tests** (`test_model.py`)
   - Performance thresholds (RMSE < 5.0, RÂ² > 0.7)
   - Feature importance
   - Model invariants (mÃ¡s rooms â†’ mayor precio)
   - Scaler properties

---

## ğŸ”® Mejoras Futuras

### Corto Plazo (1-2 sprints)

1. **Data Quality Checks**
   - Implementar Great Expectations
   - ValidaciÃ³n automÃ¡tica de datos incoming
   - Alertas en data quality issues

2. **A/B Testing**
   - Endpoint para mÃºltiples modelos
   - Traffic splitting configurable
   - ComparaciÃ³n de performance

3. **Explicabilidad**
   - Endpoint `/explain` con SHAP values
   - Feature contribution por predicciÃ³n

### Mediano Plazo (3-6 meses)

4. **Model Registry Avanzado**
   - Staging â†’ Production automÃ¡tico
   - Rollback capabilities
   - Modelo champion/challenger

5. **Monitoring Avanzado**
   - Prometheus + Grafana stack
   - Alerting basado en mÃ©tricas
   - Dashboards personalizados

6. **Reentrenamiento Inteligente**
   - Trigger basado en drift detection
   - Reentrenamiento con datos nuevos
   - EvaluaciÃ³n automÃ¡tica pre-deploy

### Largo Plazo (6+ meses)

7. **Kubernetes Deployment**
   - Helm charts para deployment
   - Autoscaling basado en carga
   - Multi-region deployment

8. **Feature Store**
   - CentralizaciÃ³n de features
   - Online/Offline serving
   - Feature lineage tracking

9. **AutoML Integration**
   - Hyperparameter optimization automÃ¡tico
   - Model selection automÃ¡tico
   - Feature engineering automÃ¡tico

---

## ğŸ¤– Uso de Herramientas AI

Este proyecto fue desarrollado con asistencia de herramientas de IA para maximizar productividad y calidad:

### GitHub Copilot

**Uso principal:**
- Autocompletado de cÃ³digo repetitivo (logging, docstrings)
- Sugerencias de tests basados en funciones existentes
- GeneraciÃ³n de schemas Pydantic

**Impacto:**
- âš¡ 40% mÃ¡s rÃ¡pido en escribir tests
- ğŸ“ DocumentaciÃ³n mÃ¡s consistente
- ğŸ› Menos bugs por typos

### ChatGPT / Claude

**Uso principal:**
- RevisiÃ³n de arquitectura y decisiones tÃ©cnicas
- OptimizaciÃ³n de prompts para documentaciÃ³n
- Debugging de issues complejos (ej: scaler dict problem)

**Ejemplo concreto:**
```
Problema: API fallaba con "dict has no attribute transform"
SoluciÃ³n con AI: IdentificÃ³ que scaler estaba guardado como dict
â†’ CambiÃ³ arquitectura para guardar StandardScaler puro
```

### Consideraciones Ã‰ticas

- âœ… Todo el cÃ³digo generado fue **revisado y validado**
- âœ… Se **entiende completamente** la lÃ³gica implementada
- âœ… Tests garantizan **correctitud** del cÃ³digo
- âœ… DocumentaciÃ³n refleja **decisiones conscientes**, no solo output de AI

---

## ğŸ“ Licencia

MIT License - ver [LICENSE](LICENSE) para detalles

---

## ğŸ‘¤ Autor

**Mateo Restrepo**
- GitHub: [@marestrepohi](https://github.com/marestrepohi)
- LinkedIn: [mateo-restrepo](https://linkedin.com/in/mateo-restrepo)

---

## ğŸ™ Agradecimientos

- Dataset: Boston Housing (UCI Machine Learning Repository)
- MLflow Team por el excelente framework de tracking
- FastAPI team por la mejor experiencia de developer
- Comunidad open-source de Python ML

---

## ğŸ“š Referencias

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)
- [DVC Documentation](https://dvc.org/doc)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [MLOps Principles](https://ml-ops.org/)

---

**â­ Si este proyecto te fue Ãºtil, considera darle una estrella en GitHub!**
