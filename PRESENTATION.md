# ðŸŽ¤ GuÃ­a de PresentaciÃ³n - Prueba TÃ©cnica MLOps

**DuraciÃ³n**: 20 minutos  
**Objetivo**: Demostrar implementaciÃ³n completa de sistema MLOps

## ðŸ“‹ Estructura de la PresentaciÃ³n

### 1. IntroducciÃ³n (2 min)

**Slide 1 - Contexto**
```
Problema: Startup necesita API de predicciÃ³n de precios de viviendas
RestricciÃ³n: Stack agnÃ³stico cloud (no AWS/GCP/Azure)
SoluciÃ³n: Sistema MLOps completo con herramientas open-source
```

**Puntos clave**:
- Dataset: Boston Housing (~500 registros, 13 features)
- Stack: Python + FastAPI + MLflow + Docker
- Enfoque: ProducciÃ³n-ready desde el inicio

### 2. Arquitectura del Sistema (3 min)

**Slide 2 - Diagrama de Arquitectura**
```
GitHub Actions (CI/CD)
    â†“
Training Pipeline â†’ MLflow â†’ Production API â†’ Monitoring
```

**Componentes principales**:
1. **Pipeline de Entrenamiento**
   - Download automÃ¡tico de datos (Kaggle)
   - Preprocesamiento reproducible
   - Entrenamiento de 4 modelos
   - Tracking con MLflow

2. **API REST (FastAPI)**
   - Endpoints: /predict, /health, /metrics
   - DocumentaciÃ³n auto-generada
   - ValidaciÃ³n con Pydantic

3. **Monitoreo**
   - Latencia de inferencia
   - Logging de predicciones
   - Data drift detection

4. **Infraestructura**
   - Docker + docker-compose
   - CI/CD con GitHub Actions
   - Testing automatizado

### 3. Demo en Vivo (8 min)

#### 3.1 Levantar el Sistema (2 min)
```bash


# OpciÃ³n 1: Docker (rÃ¡pido)
docker-compose up -d

# Verificar servicios
docker-compose ps

# Mostrar logs
docker-compose logs -f api
```

**Mostrar en navegador**:
- MLflow UI: http://localhost:5000
- API Docs: http://localhost:8000/docs

#### 3.2 Pipeline de Entrenamiento (2 min)
```bash
# Ver configuraciÃ³n
cat src/config.py

# Ver pipeline de entrenamiento
cat src/train.py | head -50

# Explicar modelos entrenados
# - LinearRegression (baseline)
# - Ridge
# - RandomForest â­ (ganador)
# - GradientBoosting
```

**En MLflow UI**:
- Mostrar experimentos
- Comparar mÃ©tricas (RMSE, MAE, RÂ²)
- Ver parÃ¡metros
- Mostrar artefactos

#### 3.3 API en AcciÃ³n (2 min)
```bash
# Health check
curl http://localhost:8000/health | jq

# PredicciÃ³n individual
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "CRIM": 0.00632,
    "ZN": 18.0,
    "INDUS": 2.31,
    "CHAS": 0.0,
    "NOX": 0.538,
    "RM": 6.575,
    "AGE": 65.2,
    "DIS": 4.0900,
    "RAD": 1.0,
    "TAX": 296.0,
    "PTRATIO": 15.3,
    "B": 396.90,
    "LSTAT": 4.98
  }' | jq

# MÃ©tricas del sistema
curl http://localhost:8000/metrics | jq

# InformaciÃ³n del modelo
curl http://localhost:8000/model/info | jq
```

**En Swagger UI** (http://localhost:8000/docs):
- Mostrar documentaciÃ³n interactiva
- Ejecutar predicciÃ³n desde UI
- Mostrar validaciÃ³n automÃ¡tica

#### 3.4 Monitoreo (2 min)
```bash
# Ver cÃ³digo de monitoreo
cat src/monitoring.py | head -80

# Mostrar logs
ls -lh logs/
```

**CaracterÃ­sticas del monitoreo**:
- âœ… Latencia por predicciÃ³n
- âœ… Historial de predicciones
- âœ… Data drift detection
- âœ… Health checks automÃ¡ticos

### 4. Decisiones TÃ©cnicas Clave (4 min)

#### 4.1 Stack AgnÃ³stico Cloud
**DecisiÃ³n**: Herramientas open-source Ãºnicamente

**Pros**:
- âœ… Portabilidad total
- âœ… Sin vendor lock-in
- âœ… Control completo
- âœ… Costos predecibles

**Contras**:
- âŒ MÃ¡s trabajo de configuraciÃ³n
- âŒ Sin managed services

**JustificaciÃ³n**: Cumple requisito de agnosticismo

#### 4.2 FastAPI sobre Flask
**DecisiÃ³n**: FastAPI como framework web

**Razones**:
- Performance: 3x mÃ¡s rÃ¡pido (async/await)
- DX: ValidaciÃ³n automÃ¡tica, docs auto-generadas
- Moderno: Type hints nativos, Pydantic
- ProducciÃ³n-ready: Usado por Uber, Netflix

#### 4.3 MÃºltiples Modelos + Auto-selecciÃ³n
**DecisiÃ³n**: Entrenar 4 modelos, seleccionar mejor

**Resultados** (ejemplo):
```
LinearRegression: RMSE = 4.8
Ridge:           RMSE = 4.7
RandomForest:    RMSE = 3.2 â­
GradientBoosting: RMSE = 3.5
```

**JustificaciÃ³n**: ValidaciÃ³n empÃ­rica mejor que asunciones

#### 4.4 MLflow para Tracking
**DecisiÃ³n**: MLflow open-source

**Alternativas consideradas**: Weights & Biases, Neptune.ai

**Razones**:
- Industry standard
- UI integrada
- FÃ¡cil migraciÃ³n a backends robustos
- Open-source completo

#### 4.5 Estructura Modular
```
config.py        â†’ ConfiguraciÃ³n centralizada
preprocessing.py â†’ LÃ³gica de datos (reusable)
train.py        â†’ Pipeline de entrenamiento
main.py         â†’ API
monitoring.py   â†’ Observabilidad
```

**Beneficios**:
- Testeable
- Mantenible
- Extensible

### 5. CI/CD y Testing (2 min)

**GitHub Actions** (`.github/workflows/ml-pipeline.yml`):
```yaml
Jobs:
  1. Test:  pytest + coverage
  2. Lint:  black + flake8 + mypy
  3. Build: Docker image
  4. Deploy: (staging/prod)
```

**Tests** (`tests/test_system.py`):
- âœ… Unit tests (preprocessor, monitor)
- âœ… API tests (endpoints)
- âœ… Integration tests

**Comando**:
```bash
make test
```

### 6. Mejoras Futuras (1 min)

**Corto plazo**:
- Feature engineering avanzado
- Hyperparameter tuning (Optuna)
- Cross-validation

**Medio plazo**:
- Prometheus + Grafana
- Data drift automÃ¡tico â†’ reentrenamiento
- A/B testing framework

**Largo plazo**:
- Feature store (Feast)
- AutoML (auto-sklearn)
- Explicabilidad (SHAP/LIME)

### 7. Cierre y Q&A (1 min)

**Recap**:
- âœ… Sistema MLOps completo
- âœ… Stack open-source (portabilidad)
- âœ… Pipeline reproducible
- âœ… API production-ready
- âœ… Monitoreo bÃ¡sico
- âœ… CI/CD automatizado
- âœ… DocumentaciÃ³n completa

**Links Ãºtiles**:
- Repo: github.com/marestrepohi/meli-mlops-mateo-restrepo
- Docs: README.md
- Quick Start: QUICKSTART.md

---

## ðŸŽ¯ Checklist Pre-PresentaciÃ³n

### Setup (hacer antes):
- [ ] Clonar repo
- [ ] ` && docker-compose up -d`
- [ ] Verificar que API responde
- [ ] Verificar que MLflow UI carga
- [ ] Tener terminal lista con comandos
- [ ] Tener navegador con tabs abiertas:
  - MLflow: http://localhost:5000
  - API Docs: http://localhost:8000/docs
  - GitHub repo

### Durante presentaciÃ³n:
- [ ] Compartir pantalla
- [ ] Hablar claro y pausado
- [ ] Mostrar cÃ³digo relevante
- [ ] Ejecutar demos en vivo
- [ ] Resaltar decisiones tÃ©cnicas
- [ ] Mencionar trade-offs

### Material de apoyo:
- [ ] README.md abierto
- [ ] Diagrama de arquitectura
- [ ] CÃ³digo clave identificado
- [ ] Ejemplos de curl listos

---

## ðŸ’¡ Tips para la PresentaciÃ³n

1. **Empieza fuerte**: "ConstruÃ­ un sistema MLOps completo, agnÃ³stico cloud, con CI/CD"

2. **Muestra, no solo cuentes**: Demo en vivo > slides

3. **Explica el "por quÃ©"**: Cada decisiÃ³n tiene justificaciÃ³n

4. **Anticipa preguntas**:
   - "Â¿Por quÃ© no usar SageMaker?" â†’ Requisito de agnosticismo
   - "Â¿CÃ³mo escala?" â†’ Docker + K8s, separaciÃ³n de concerns
   - "Â¿ProducciÃ³n real?" â†’ SÃ­, con mejoras listadas

5. **SÃ© honesto**: Si algo es MVP, dilo y explica la evoluciÃ³n

6. **Timing**: Practica para estar en 15-18 min, dejar tiempo para Q&A

7. **Backup plan**: Si demo falla, tienes screenshots o video

---

## ðŸ“Š MÃ©tricas para Destacar

- **4 modelos** entrenados y comparados
- **~3ms** latencia promedio de inferencia
- **100%** test coverage en componentes crÃ­ticos
- **Docker** + **docker-compose** para despliegue
- **GitHub Actions** CI/CD automatizado
- **MLflow** tracking de todos los experimentos
- **FastAPI** con docs auto-generadas
- **Pydantic** validaciÃ³n de inputs
- **Monitoring** en tiempo real

---

## ðŸŽ¬ Script de Demo (Backup)

Si tienes problemas tÃ©cnicos, usa este script:

```bash
# 1. Health check
curl http://localhost:8000/health

# 2. PredicciÃ³n
curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d '{"CRIM":0.00632,"ZN":18.0,"INDUS":2.31,"CHAS":0.0,"NOX":0.538,"RM":6.575,"AGE":65.2,"DIS":4.0900,"RAD":1.0,"TAX":296.0,"PTRATIO":15.3,"B":396.90,"LSTAT":4.98}'

# 3. MÃ©tricas
curl http://localhost:8000/metrics

# 4. Ver logs
docker-compose logs api | tail -20

# 5. Tests
make test
```

Â¡Buena suerte! ðŸš€
