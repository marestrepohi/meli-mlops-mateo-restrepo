# ğŸ¯ PresentaciÃ³n TÃ©cnica - MLOps Production System
## Housing Price Prediction

**DuraciÃ³n:** 20 minutos  
**Fecha:** TBD  
**Presentador:** Mateo Restrepo

---

## ğŸ“Š Agenda (20 minutos)

1. **IntroducciÃ³n** (2 min)
2. **Arquitectura del Sistema** (4 min)
3. **Pipeline de ML** (4 min)
4. **API y Monitoreo** (4 min)
5. **CI/CD y Deployment** (3 min)
6. **Decisiones TÃ©cnicas** (2 min)
7. **Q&A** (1 min)

---

## 1. ğŸ¬ IntroducciÃ³n (2 min)

### Problema de Negocio
> "Startup necesita predecir precios de viviendas como servicio accesible via API REST"

### SoluciÃ³n Implementada
- âœ… Pipeline reproducible de ML
- âœ… API REST con FastAPI
- âœ… Monitoreo en producciÃ³n
- âœ… CI/CD automatizado
- âœ… 100% Open-Source (agnÃ³stico a cloud)

### Stack TecnolÃ³gico
```
Python + XGBoost + MLflow + FastAPI + Docker + DVC + GitHub Actions
```

**Demo rÃ¡pida:** `curl http://localhost:8000/predict`

---

## 2. ğŸ—ï¸ Arquitectura del Sistema (4 min)

### Vista de Alto Nivel

```
Data â†’ Pipeline (DVC) â†’ MLflow â†’ Model Artifacts â†’ API â†’ Users
  â†“                        â†“                           â†“
  Raw                  Experiments              Predictions
                           â†“                           â†“
                      Best Model               Monitoring
```

### Componentes Principales

#### A. Pipeline de Datos (DVC)
```yaml
data_ingestion â†’ data_preparation â†’ model_train
     â†“                  â†“                 â†“
  raw/*.csv      processed/*.csv    mlruns/ + models/
```

#### B. Tracking de Experimentos (MLflow)
- 3 experimentos XGBoost automÃ¡ticos
- Autologging de mÃ©tricas y artefactos
- Model Registry (opcional)

#### C. API REST (FastAPI)
- Endpoints: `/predict`, `/predict/batch`, `/health`, `/metrics`, `/drift`
- ValidaciÃ³n automÃ¡tica (Pydantic)
- AutenticaciÃ³n + Rate limiting

#### D. Monitoreo
- Performance metrics (latency, throughput)
- Data drift detection
- Model performance tracking

**Punto clave:** SeparaciÃ³n clara de responsabilidades, single source of truth

---

## 3. ğŸ”¬ Pipeline de ML (4 min)

### Etapa 1: Data Ingestion (`src/data_ingestion.py`)
```python
Kaggle API â†’ HousingData.csv â†’ EDA Report (ydata-profiling)
```
- 506 muestras, 14 variables (13 features + 1 target)
- GeneraciÃ³n automÃ¡tica de EDA

### Etapa 2: Data Preparation (`src/data_preparation.py`)
```python
Raw Data â†’ Clean â†’ Split (80/20) â†’ Scale â†’ Processed + Scaler
```

### Etapa 3: Model Training (`src/model_train.py`)

**3 Experimentos AutomÃ¡ticos:**

1. **Hyperparameter Tuning** (todas las features)
   - RandomizedSearchCV, 50 iteraciones
   - Baseline: RMSE ~3.5, RÂ² ~0.85

2. **Important Features** (SHAP selection)
   - Top 10 features (percentil 20)
   - Default params
   - SelecciÃ³n: CRIM, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT

3. **Tuning on Selected Features** (BEST)
   - RandomizedSearchCV en features seleccionadas
   - **Result: RMSE 2.46, RÂ² 0.917** âœ…

**MLflow Autologging:**
- HiperparÃ¡metros
- MÃ©tricas (train/test/cv)
- Model signature
- Artifacts (plots, SHAP values)

**Export a ProducciÃ³n:**
```python
models/production/latest/
â”œâ”€â”€ model.pkl         # XGBoost model
â”œâ”€â”€ scaler.pkl        # StandardScaler (desde data_preparation)
â””â”€â”€ metadata.json     # Features, metrics, run_id
```

**Punto clave:** Reproducibilidad total con `dvc repro`

---

## 4. ğŸŒ API y Monitoreo (4 min)

### FastAPI - DiseÃ±o de la API

#### Endpoint Principal: `POST /predict`

**Input (Pydantic validation):**
```json
{
  "CRIM": 0.00632,  // Required, range [0, 100]
  "NOX": 0.538,     // Required, range [0.3, 1.0]
  "RM": 6.575,      // Required, range [3, 9]
  // ... 7 mÃ¡s requeridas
  "ZN": 18.0,       // Optional (para scaler)
  "INDUS": 2.31,    // Optional
  "CHAS": 0.0       // Optional
}
```

**Output:**
```json
{
  "prediction": 24.5,
  "model_name": "boston_housing_xgboost",
  "inference_time": 15.2,
  "features_used": ["CRIM", "NOX", ...]
}
```

**Flujo interno:**
1. ValidaciÃ³n Pydantic (rangos, tipos)
2. ConstrucciÃ³n de array con todas las 13 features
3. `scaler.transform()` â†’ datos escalados
4. `model.predict()` â†’ predicciÃ³n
5. Logging para monitoreo

#### Batch Predictions: `POST /predict/batch`
```json
{
  "instances": [
    {"CRIM": 0.00632, ...},
    {"CRIM": 0.02731, ...}
  ]
}
```

### Monitoreo en ProducciÃ³n

**MÃ³dulo:** `api/monitoring.py`

#### MÃ©tricas rastreadas:
1. **Performance:**
   - Total predicciones
   - Uptime
   - Requests/hora

2. **Latency:**
   - Promedio: ~15ms
   - p95: <50ms
   - p99: <100ms

3. **Data Drift Detection:**
```python
drift_score = |current_mean - baseline_mean| / baseline_std
if drift_score > 2.0:
    alert("Data drift detected!")
```

**Endpoint de mÃ©tricas:** `GET /metrics`

### Seguridad Implementada

1. **API Key Authentication**
```bash
curl -H "X-API-Key: demo-key-123" ...
```

2. **Rate Limiting**
- 100 requests/min por API key
- Responde 429 Too Many Requests

3. **Input Validation**
- Rangos vÃ¡lidos por feature
- Tipos correctos
- Errores 422 con detalles

**Punto clave:** Balance entre seguridad y facilidad de uso

---

## 5. ğŸš€ CI/CD y Deployment (3 min)

### GitHub Actions - 3 Workflows

#### 1. `ml-pipeline.yml` (Main Pipeline)
```yaml
on: [push, pull_request]

jobs:
  test:
    - Setup Python
    - Install dependencies
    - Run pytest
  
  train:
    - Setup DVC
    - dvc repro
    - Upload artifacts
  
  deploy:
    - Build Docker image
    - Push to registry
    - Deploy to production
```

#### 2. `api-tests.yml` (API Testing)
```yaml
on: [pull_request]

jobs:
  test-api:
    - Unit tests
    - Integration tests
    - Performance tests
```

#### 3. `scheduled-retrain.yml` (Automated Retraining)
```yaml
on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly

jobs:
  retrain:
    - Pull latest data
    - Run dvc repro
    - Evaluate new model
    - Deploy if better
```

### Docker - ContainerizaciÃ³n

**Dockerfile:**
```dockerfile
FROM python:3.10-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0"]
```

**docker-compose.yml:**
```yaml
services:
  api:
    build: .
    ports: ["8000:8000"]
  
  mlflow:
    image: ghcr.io/mlflow/mlflow
    ports: ["5000:5000"]
```

**Deploy:**
```bash
docker-compose up -d
# API â†’ http://localhost:8000
# MLflow UI â†’ http://localhost:5000
```

**Punto clave:** Portabilidad total - corre en cualquier parte

---

## 6. ğŸ§  Decisiones TÃ©cnicas (2 min)

### 1. XGBoost vs Otros Modelos

**Â¿Por quÃ© XGBoost?**
- âœ… Mejor performance en datos tabulares
- âœ… Feature importance nativo
- âœ… Compatible con SHAP para explicabilidad
- âœ… RÃ¡pido en inferencia

**Alternativas consideradas:**
- Random Forest: MÃ¡s lento, similar performance
- Neural Networks: Overkill para 13 features
- Linear Regression: RMSE 4.8 (vs 2.46 con XGBoost)

### 2. FastAPI vs Flask

**Â¿Por quÃ© FastAPI?**
- âœ… Async by default (mejor concurrencia)
- âœ… ValidaciÃ³n automÃ¡tica (Pydantic)
- âœ… Docs autogeneradas (Swagger)
- âœ… Type hints nativos

### 3. MLflow Open-Source vs SageMaker/Azure ML

**Â¿Por quÃ© MLflow?**
- âœ… AgnÃ³stico a cloud (requisito)
- âœ… Open-source
- âœ… Self-hosted
- âœ… Portabilidad total

### 4. Scaler: Â¿Dict o StandardScaler puro?

**Problema original:**
```python
# âŒ Guardaba dict {"escalador": scaler, ...}
# API fallaba: "dict has no attribute transform"
```

**SoluciÃ³n:**
```python
# âœ… Guardar StandardScaler puro
joblib.dump(prep.escalador, "scaler.pkl")
```

**Beneficio:** API mÃ¡s simple, menos overhead

### 5. Monitoreo: In-Memory vs Persistente

**SoluciÃ³n actual:** In-memory (deque)
- âœ… RÃ¡pido, sin overhead
- âœ… Suficiente para MVP
- âŒ Se pierde al restart

**ProducciÃ³n real:** Prometheus + Grafana + TimeSeries DB

### 6. Testing: Unit + Integration + Performance

**Cobertura completa:**
```bash
pytest --cov=src --cov=api
# Coverage: 85%
```

**Tipos de tests:**
- Unit: Funciones individuales
- Integration: Pipeline end-to-end
- Performance: Latencia < 100ms
- Model Quality: RMSE < 5.0, RÂ² > 0.7

---

## 7. ğŸ’¡ Lecciones Aprendidas

### Lo que funcionÃ³ bien âœ…
1. **DVC para reproducibilidad**: `dvc repro` = pipeline completo
2. **MLflow Autologging**: 80% menos cÃ³digo de tracking
3. **Pydantic validation**: Errores claros desde el dÃ­a 1
4. **Docker Compose**: Setup en 2 comandos

### DesafÃ­os enfrentados ğŸ”§
1. **Scaler dict issue**: Debugging con AI (ChatGPT)
2. **DVC outputs duplicados**: Pipeline re-ejecutaba innecesariamente
3. **GitHub Actions secrets**: ConfiguraciÃ³n de API keys

### PrÃ³ximos pasos ğŸš€
1. **Data Quality**: Great Expectations
2. **A/B Testing**: Champion/Challenger models
3. **Kubernetes**: Deployment escalable
4. **Feature Store**: CentralizaciÃ³n de features

---

## ğŸ“Š Resultados Cuantitativos

### Performance del Modelo
- **Test RMSE:** 2.46 (objetivo: <5.0) âœ…
- **Test RÂ²:** 0.917 (objetivo: >0.7) âœ…
- **Features seleccionadas:** 10/13 (23% reducciÃ³n) âœ…

### Performance de la API
- **Latency promedio:** 15ms âœ…
- **p95 latency:** <50ms âœ…
- **Throughput:** ~300 req/s (local) âœ…

### Cobertura de Tests
- **Unit tests:** 45 tests passing âœ…
- **Coverage:** 85% âœ…
- **CI/CD:** 100% automatizado âœ…

---

## ğŸ™‹ Preguntas Frecuentes

### Q: Â¿CÃ³mo escalar a millones de requests?
**A:** 
1. Kubernetes con HPA (Horizontal Pod Autoscaler)
2. Load balancer (nginx/HAProxy)
3. Redis para caching de predicciones frecuentes
4. Model serving optimizado (ONNX, TensorRT)

### Q: Â¿CÃ³mo manejar model degradation?
**A:**
1. Monitoreo continuo de mÃ©tricas
2. Drift detection automÃ¡tico
3. Reentrenamiento triggered por drift
4. A/B testing de nuevo modelo
5. Rollback automÃ¡tico si falla

### Q: Â¿Seguridad en producciÃ³n?
**A:**
1. API Keys en secrets manager (Vault)
2. HTTPS/TLS obligatorio
3. Rate limiting por cliente
4. WAF (Web Application Firewall)
5. Logging de auditorÃ­a

### Q: Â¿CÃ³mo agregar nuevas features?
**A:**
1. Actualizar `data_preparation.py`
2. Re-entrenar modelo con `dvc repro`
3. Tests automÃ¡ticos verifican compatibilidad
4. Deploy gradual con A/B testing

---

## ğŸ“ Conclusiones

### Logros del Proyecto
âœ… **Pipeline reproducible** con DVC  
âœ… **API REST productiva** con FastAPI  
âœ… **Monitoreo completo** (latency, drift, performance)  
âœ… **CI/CD automatizado** con GitHub Actions  
âœ… **100% Open-Source** (agnÃ³stico a cloud)  
âœ… **DocumentaciÃ³n completa** + tests comprehensivos  

### Valor de Negocio
- **Time to market:** <2 semanas
- **Cost:** $0 en servicios cloud
- **Maintenance:** Automatizado 90%
- **Scalability:** Ready para K8s
- **Extensibility:** Arquitectura modular

### VisiÃ³n a Futuro
Este sistema es la **base** para:
- Multiple models (diferentes regiones)
- Feature store (features compartidas)
- AutoML pipeline (optimizaciÃ³n continua)
- Multi-tenant API (varios clientes)

---

## ğŸ¯ Puntos Clave para Recordar

1. **Arquitectura agnÃ³stica a cloud** â†’ Portabilidad total
2. **MLflow Autologging** â†’ Menos cÃ³digo, mÃ¡s valor
3. **DVC para reproducibilidad** â†’ `dvc repro` es magia
4. **FastAPI + Pydantic** â†’ ValidaciÃ³n automÃ¡tica
5. **Monitoreo desde dÃ­a 1** â†’ Drift detection incluido
6. **CI/CD automatizado** â†’ GitHub Actions FTW
7. **Tests comprehensivos** â†’ 85% coverage
8. **Docker everything** â†’ Deployment sin fricciÃ³n

---

## ğŸ“š Material de Soporte

- **CÃ³digo:** https://github.com/marestrepohi/meli-mlops-mateo-restrepo
- **Docs:** Swagger UI en `/docs`
- **MLflow:** Tracking UI en `:5000`
- **Tests:** `pytest -v`

---

**Â¿Preguntas? ğŸ™‹**

---

## ğŸ Bonus: Demo en Vivo

```bash
# 1. Health check
curl http://localhost:8000/health

# 2. PredicciÃ³n simple
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"CRIM": 0.00632, "NOX": 0.538, "RM": 6.575, ...}'

# 3. Ver mÃ©tricas
curl http://localhost:8000/metrics

# 4. Drift detection
curl http://localhost:8000/drift

# 5. MLflow UI
open http://localhost:5000
```

---

**Gracias por su atenciÃ³n! ğŸ™**

*"MLOps no es solo cÃ³digo, es cultura de deployment continuo con calidad."*
